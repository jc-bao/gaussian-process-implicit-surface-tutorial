{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyTorch Regression Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cube\n",
    "xs = torch.linspace(-1, 1, steps=11)\n",
    "ys = torch.linspace(-1, 1, steps=11)\n",
    "zs = torch.linspace(-1, 1, steps=11)\n",
    "\n",
    "xp_face = torch.stack(torch.meshgrid(torch.ones(1), ys, zs), dim=-1).view(-1, 3)\n",
    "xn_face = torch.stack(torch.meshgrid(-torch.ones(1), ys, zs), dim=-1).view(-1, 3)\n",
    "yp_face = torch.stack(torch.meshgrid(xs, torch.ones(1), zs), dim=-1).view(-1, 3)\n",
    "yn_face = torch.stack(torch.meshgrid(xs, -torch.ones(1), zs), dim=-1).view(-1, 3)\n",
    "zp_face = torch.stack(torch.meshgrid(xs, ys, torch.ones(1)), dim=-1).view(-1, 3)\n",
    "zn_face = torch.stack(torch.meshgrid(xs, ys, -torch.ones(1)), dim=-1).view(-1, 3)\n",
    "\n",
    "cube = torch.cat(\n",
    "  (xp_face, xn_face, yp_face, yn_face, zp_face, zn_face), dim=0) \n",
    "cube_noisy = torch.randn_like(cube) * 0.05 + cube\n",
    "outer = cube * 1.5\n",
    "inner = torch.zeros((1,3))\n",
    "\n",
    "x_train = torch.cat((cube_noisy, outer, inner), dim=0)\n",
    "\n",
    "f_train = torch.cat([\n",
    "  torch.zeros(cube_noisy.shape[0]),\n",
    "  torch.ones(outer.shape[0]),\n",
    "  -torch.ones(inner.shape[0])\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.set_zlim([-2, 2])\n",
    "ax.scatter(cube_noisy[:,0].numpy(), cube_noisy[:,1].numpy(), cube_noisy[:,2].numpy(), c='b', marker='o')\n",
    "ax.scatter(inner[:,0].numpy(), inner[:,1].numpy(), inner[:,2].numpy(), c='r', marker='x')\n",
    "# ax.scatter(outer[:,0].numpy(), outer[:,1].numpy(), outer[:,2].numpy(),  c='g', marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinPlateRegularizer(gpytorch.kernels.Kernel):\n",
    "  # the sinc kernel is stationary\n",
    "  is_stationary = True\n",
    "\n",
    "  # We will register the parameter when initializing the kernel\n",
    "  def __init__(self, dist_prior=None, dist_constraint=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.register_parameter(\n",
    "      name='max_dist', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1))\n",
    "    )\n",
    "    if dist_constraint is None:\n",
    "      dist_constraint = gpytorch.constraints.GreaterThan(0.20)\n",
    "    self.register_constraint(\"max_dist\", dist_constraint)\n",
    "    if dist_prior is not None:\n",
    "      self.register_prior(\n",
    "        \"dist_prior\",\n",
    "        dist_prior,\n",
    "        lambda m: m.length,\n",
    "        lambda m, v: m._set_length(v),\n",
    "      )\n",
    "\n",
    "  @property\n",
    "  def maxdist(self):\n",
    "    return self.raw_dist_constraint.transform(self.max_dist)\n",
    "\n",
    "  @maxdist.setter\n",
    "  def maxdist(self, value):\n",
    "    return self._set_maxdist(value)\n",
    "\n",
    "  def _set_maxdist(self, value):\n",
    "    if not torch.is_tensor(value):\n",
    "      value = torch.as_tensor(value).to(self.max_dist)\n",
    "    self.initialize(max_dist=self.raw_dist_constraint.inverse_transform(value))\n",
    "\n",
    "  # this is the kernel function\n",
    "  def forward(self, x1, x2, **params):\n",
    "    diff = self.covar_dist(x1, x2, **params)\n",
    "    diff.where(diff == 0, torch.as_tensor(1e-20))\n",
    "    noise = 1e-5\n",
    "    white = noise*torch.eye(diff.shape[0], diff.shape[1])\n",
    "    tp = 2*torch.pow(diff, 3)-3*self.max_dist * \\\n",
    "      torch.pow(diff, 2)+self.max_dist**3\n",
    "    return white+tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class thinPlateModel(gpytorch.models.ExactGP):\n",
    "  def __init__(self, train_x, train_y, likelihood):\n",
    "    super(thinPlateModel, self).__init__(train_x, train_y, likelihood)\n",
    "    self.mean_module = gpytorch.means.ZeroMean()\n",
    "    self.covar_module = ThinPlateRegularizer()\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean_x = self.mean_module(x)\n",
    "    covar_x = self.covar_module(x)\n",
    "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = thinPlateModel(x_train, f_train, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "  'likelihood.noise_covar.noise': torch.tensor(0.05),\n",
    "  'covar_module.max_dist': torch.tensor(8.0),\n",
    "}\n",
    "model_params = model.initialize(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in (pbar := trange(training_iter)):\n",
    "  # Zero gradients from previous iteration\n",
    "  optimizer.zero_grad()\n",
    "  # Output from model\n",
    "  output = model(x_train)\n",
    "  loss = -mll(output, f_train)\n",
    "  loss.backward()\n",
    "  pbar.set_description('Iter %d/%d - Loss: %.3f' % (\n",
    "    i + 1, training_iter, loss.item(),\n",
    "  ))\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolusion = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "xs = torch.linspace(-1.5, 1.5, steps=resolusion + 1)\n",
    "ys = torch.linspace(-1.5, 1.5, steps=resolusion + 1)\n",
    "zs = torch.linspace(-1.5, 1.5, steps=resolusion + 1)\n",
    "x_test = torch.stack(torch.meshgrid(xs, ys, zs), dim=-1)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "  f_test = likelihood(model(x_test.view(-1, 3)))\n",
    "  \n",
    "f_test_mu = f_test.mean.view(resolusion+1, resolusion+1, resolusion+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "f_test_mu = torch.clip(f_test_mu, -1, 1)\n",
    "\n",
    "pp = torch.linspace(0,resolusion,resolusion+1)\n",
    "ii, jj = np.meshgrid(pp, pp)\n",
    "\n",
    "f_test_z = f_test_mu[:,:,resolusion//2].numpy()\n",
    "sns.heatmap(f_test_z, square=True,  cmap=\"RdBu\", ax=axes[0])\n",
    "mask = (np.abs(f_test_z) < 1e-2)\n",
    "axes[0].scatter(ii[mask], jj[mask], color='g')\n",
    "\n",
    "f_test_y = f_test_mu[:,resolusion//2,:].numpy()\n",
    "ax = sns.heatmap(f_test_y, square=True,  cmap=\"RdBu\", ax=axes[1])\n",
    "mask = (np.abs(f_test_y) < 1e-2)\n",
    "axes[1].scatter(ii[mask], jj[mask], color='g')\n",
    "\n",
    "f_test_x = f_test_mu[:,resolusion//2,:].numpy()\n",
    "ax = sns.heatmap(f_test_x, square=True,  cmap=\"RdBu\", ax=axes[2])\n",
    "mask = (np.abs(f_test_x) < 1e-2)\n",
    "axes[2].scatter(ii[mask], jj[mask], color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "mask = (np.abs(f_test_mu) < 1e-2)\n",
    "surface = x_test[mask,:]\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(surface[:,0], surface[:,1], surface[:,2], c='g', alpha=0.3, linewidths=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf58728769140bed469c3f1c7517edbf13ffa7b6f4a744745895768c3bf5c554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
