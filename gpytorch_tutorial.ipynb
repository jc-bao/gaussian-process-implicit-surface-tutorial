{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyTorch Regression Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cube\n",
    "xs = torch.linspace(-1, 1, steps=10)\n",
    "ys = torch.linspace(-1, 1, steps=10)\n",
    "\n",
    "# suface points\n",
    "x_face = torch.cat([\n",
    "  xs,\n",
    "  torch.ones_like(ys),\n",
    "  xs, \n",
    "  -torch.ones_like(ys)\n",
    "])\n",
    "x_face_noise = x_face + torch.randn_like(x_face) * 0.05\n",
    "y_face = torch.cat([\n",
    "  torch.ones_like(ys),\n",
    "  ys,\n",
    "  -torch.ones_like(ys),\n",
    "  ys\n",
    "])\n",
    "y_face_noise = y_face + torch.randn_like(x_face) * 0.05\n",
    "\n",
    "# outer points\n",
    "x_out = x_face * 1.5\n",
    "y_out = y_face * 1.5\n",
    "\n",
    "# inner points\n",
    "x_in = torch.zeros(1)\n",
    "y_in = torch.zeros(1)\n",
    "\n",
    "# training data\n",
    "x_train = torch.cat([\n",
    "  x_face_noise, x_out, x_in\n",
    "], dim=-1)\n",
    "y_train = torch.cat([\n",
    "  y_face_noise, y_out, y_in\n",
    "], dim=-1)\n",
    "pos_train = torch.stack([x_train, y_train], dim=-1)\n",
    "z_train = torch.cat([\n",
    "  torch.zeros_like(x_face_noise),\n",
    "  torch.ones_like(x_out),\n",
    "  -torch.ones_like(x_in)\n",
    "], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_in.numpy(), y_in.numpy(), c='r', marker='x')\n",
    "plt.scatter(x_out.numpy(), y_out.numpy(), c='b', marker='x')\n",
    "plt.scatter(x_face_noise.numpy(), y_face_noise.numpy(), c='g', marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinPlateRegularizer(gpytorch.kernels.Kernel):\n",
    "  is_stationary = True\n",
    "\n",
    "  def __init__(self, dist_prior=None, dist_constraint=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.register_parameter(\n",
    "      name=\"max_dist\",\n",
    "      parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)),\n",
    "    )\n",
    "    if dist_constraint is None:\n",
    "      dist_constraint = gpytorch.constraints.GreaterThan(0.20)\n",
    "    self.register_constraint(\"max_dist\", dist_constraint)\n",
    "    if dist_prior is not None:\n",
    "      self.register_prior(\n",
    "        \"dist_prior\",\n",
    "        dist_prior,\n",
    "        lambda m: m.length,\n",
    "        lambda m, v: m._set_length(v),\n",
    "      )\n",
    "\n",
    "    @property\n",
    "    def maxdist(self):\n",
    "      return self.raw_dist_constraint.transform(self.max_dist)\n",
    "\n",
    "    @maxdist.setter\n",
    "    def maxdist(self, value):\n",
    "      return self._set_maxdist(value)\n",
    "\n",
    "    def _set_maxdist(self, value):\n",
    "      if not torch.is_tensor(value):\n",
    "        value = torch.as_tensor(value).to(self.max_dist)\n",
    "      self.initialize(max_dist=self.raw_dist_constraint.inverse_transform(value))\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "      diff = self.covar_dist(x1, x2, **params)\n",
    "      diff.where(diff == 0, torch.as_tensor(1e-20))\n",
    "      noise = 1e-5\n",
    "      white = noise * torch.eye(diff.shape[0], diff.shape[1])\n",
    "      tp = (\n",
    "        2 * torch.pow(diff, 3)\n",
    "        - 3 * self.max_dist * torch.pow(diff, 2)\n",
    "        + self.max_dist**3\n",
    "      )\n",
    "      return white + tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class thinPlateModel(gpytorch.models.ExactGP):\n",
    "  def __init__(self, train_x, train_y, likelihood):\n",
    "    super(thinPlateModel, self).__init__(train_x, train_y, likelihood)\n",
    "    self.mean_module = gpytorch.means.ZeroMean()\n",
    "    self.covar_module = ThinPlateRegularizer()\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean_x = self.mean_module(x)\n",
    "    covar_x = self.covar_module(x)\n",
    "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = thinPlateModel(pos_train, z_train, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "  'likelihood.noise_covar.noise': torch.tensor(0.05),\n",
    "  'covar_module.max_dist': torch.tensor(0.5),\n",
    "}\n",
    "model_params = model.initialize(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "  # Zero gradients from previous iteration\n",
    "  optimizer.zero_grad()\n",
    "  # Output from model\n",
    "  output = model(pos_train)\n",
    "  # Calc loss and backprop gradients\n",
    "  output.log_prob(z_train)\n",
    "  loss = -mll(output, z_train)\n",
    "  loss.backward()\n",
    "  print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    i + 1, training_iter, loss.item(),\n",
    "    model.covar_module.base_kernel.lengthscale.item(),\n",
    "    model.likelihood.noise.item()\n",
    "  ))\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.linspace(0, 1, 100)\n",
    "f_preds = model(test_x)\n",
    "y_preds = likelihood(model(test_x))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size([1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "  test_x = torch.linspace(0, 1, 51)\n",
    "  observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  # Initialize plot\n",
    "  f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "  # Get upper and lower confidence bounds\n",
    "  lower, upper = observed_pred.confidence_region()\n",
    "  # Plot training data as black stars\n",
    "  ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "  # Plot predictive means as blue line\n",
    "  ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "  # Shade between the lower and upper confidence bounds\n",
    "  ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "  ax.set_ylim([-3, 3])\n",
    "  ax.legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf58728769140bed469c3f1c7517edbf13ffa7b6f4a744745895768c3bf5c554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
